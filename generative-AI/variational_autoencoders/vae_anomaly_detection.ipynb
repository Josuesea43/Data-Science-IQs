{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e6b920d2-3954-49ae-be02-4e51082e463b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install -q pyod==2.0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e591f31f-d304-428a-94d1-b1c78e766139",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█████████████████████████████████| 30/30 [00:00<00:00, 31.24it/s]\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from pyod.models.vae import VAE\n",
    "from pyod.models.auto_encoder import AutoEncoder\n",
    "from pyod.utils.data import (\n",
    "    generate_data, evaluate_print\n",
    ")\n",
    "from sklearn.metrics import (\n",
    "    balanced_accuracy_score, f1_score\n",
    ")\n",
    "\n",
    "plt.style.use('dark_background')\n",
    "\n",
    "# Generate synthetic data\n",
    "contamination = 0.1\n",
    "n_train = 1000\n",
    "n_test = 100\n",
    "n_features = 2\n",
    "\n",
    "X_train, X_test, y_train, y_test = generate_data(\n",
    "    n_train=n_train, n_test=n_test, \n",
    "    n_features=n_features,\n",
    "    contamination=contamination, random_state=1\n",
    ")\n",
    "\n",
    "# Train the VAE model\n",
    "clf_name_vae = 'VAE'\n",
    "vae_clf = VAE(epoch_num=30, \n",
    "              contamination=contamination, \n",
    "              beta=1.0)\n",
    "vae_clf.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "096ce6fd-665c-401b-bf16-aa280bbdd2ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█████████████████████████████████| 30/30 [00:00<00:00, 44.15it/s]\n"
     ]
    }
   ],
   "source": [
    "# Train the AE model\n",
    "clf_name_ae = 'AE'\n",
    "ae_clf = AutoEncoder(epoch_num=30, \n",
    "                     contamination=contamination)\n",
    "ae_clf.fit(X_train)\n",
    "\n",
    "# Predictions and scores for VAE\n",
    "y_test_pred_vae = vae_clf.predict(X_test)\n",
    "\n",
    "# Predictions and scores for AE\n",
    "y_test_pred_ae = ae_clf.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    balanced_accuracy_score, f1_score\n",
    ")\n",
    "\n",
    "# Compute metrics function\n",
    "def compute_metrics(y_true, y_pred):\n",
    "    balanced_acc = balanced_accuracy_score(\n",
    "        y_true, y_pred\n",
    "    )\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "    return balanced_acc, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "556a185a-a49d-4c56-9654-cbf978a06531",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_detailed_results(X, y_true, y_pred, model_name, dataset_name, ax):\n",
    "    # Compute metrics\n",
    "    balanced_acc, f1 = compute_metrics(y_true, y_pred)\n",
    "    \n",
    "    # Plot points with different categories\n",
    "    ax.scatter(X[(y_true == 1) & (y_pred == 1), 0], X[(y_true == 1) & (y_pred == 1), 1], \n",
    "               c='red', marker='x', label='True Positive (Anomaly)')\n",
    "    ax.scatter(X[(y_true == 0) & (y_pred == 0), 0], X[(y_true == 0) & (y_pred == 0), 1], \n",
    "               c='green', marker='+', label='True Negative (Non-Anomaly)')\n",
    "    ax.scatter(X[(y_true == 0) & (y_pred == 1), 0], X[(y_true == 0) & (y_pred == 1), 1], \n",
    "               c='orange', marker='*', label='False Positive (Non-Anomaly)')\n",
    "    ax.scatter(X[(y_true == 1) & (y_pred == 0), 0], X[(y_true == 1) & (y_pred == 0), 1], \n",
    "               c='blue', marker='^', label='False Negative (Anomaly)')\n",
    "    \n",
    "    # Title with metrics\n",
    "    ax.set_title(f\"{model_name} - {dataset_name}\\nBalanced Acc: {balanced_acc:.2f}, F1: {f1:.2f}\")\n",
    "    ax.set_xlabel(\"Feature 1\")\n",
    "    ax.set_ylabel(\"Feature 2\")\n",
    "    ax.legend(loc='upper left')\n",
    "\n",
    "# Create subplots\n",
    "fig, axes = plt.subplots(2, 2, figsize=(9, 9), dpi=300)\n",
    "\n",
    "# Visualize results for VAE on test data\n",
    "visualize_detailed_results(X_test, y_test, y_test_pred_vae, \"VAE\", \"Test Data\", axes[0, 0])\n",
    "\n",
    "# Visualize results for AE on test data\n",
    "visualize_detailed_results(X_test, y_test, y_test_pred_ae, \"AE\", \"Test Data\", axes[0, 1])\n",
    "\n",
    "# Visualize results for VAE on training data\n",
    "visualize_detailed_results(X_train, y_train, vae_clf.labels_, \"VAE\", \"Training Data\", axes[1, 0])\n",
    "\n",
    "# Visualize results for AE on training data\n",
    "visualize_detailed_results(X_train, y_train, ae_clf.labels_, \"AE\", \"Training Data\", axes[1, 1])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c02d901-2125-4e1e-ac6c-867fb2589388",
   "metadata": {},
   "source": [
    "$ \\text{ELBO} = \\| x - f(z) \\|^2 + \\frac{\\beta}{2} \\sum_{i=1}^{d} \\left( \\sigma_i^2 + \\mu_{q,i}^2 - 1 - \\log \\sigma_i^2 \\right) $\n",
    "\n",
    "Where:\n",
    "- First term: **Reconstruction Loss** (MSE)\n",
    "- Second term: **$ \\beta $ scaled KL Divergence** (regularizes the posterior)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb44f10f-02bf-4bf0-b73d-cccd4813a835",
   "metadata": {},
   "source": [
    "| **Aspect**                   | **Autoencoder (AE)**                                      | **Variational Autoencoder (VAE)**                          |\n",
    "|------------------------------|-----------------------------------------------------------|------------------------------------------------------------|\n",
    "| **Latent Space**              | Deterministic representation (fixed point for each input) | Probabilistic representation (distribution over latent variables) |\n",
    "| **Latent Space Structure**    | No specific regularization; can be scattered and unstructured | Regularized to match a predefined prior (typically Gaussian), resulting in a smooth, continuous space |\n",
    "| **Objective**                 | Minimize reconstruction error (e.g., MSE)                 | Minimize both reconstruction error and KL divergence to enforce latent space structure |\n",
    "| **Encoder Output**            | Direct mapping to a single point in latent space          | Outputs parameters of a distribution (mean and variance) for each latent variable |\n",
    "| **Generative Capability**     | Limited generative ability; may not generalize well for new data | Strong generative capability due to regularized latent space |\n",
    "| **Latent Variable Interpolation** | Less smooth interpolation between latent variables        | Smooth interpolation due to the continuous nature of the latent space |\n",
    "| **KL Divergence**             | Not used in the loss function                             | KL divergence term in the loss function regularizes the latent space |\n",
    "| **Reconstruction**            | Reconstructs the input deterministically                  | Reconstructs the input probabilistically, sampling from the learned latent distribution |\n",
    "| **Use Cases**                 | Mainly used for dimensionality reduction and reconstruction tasks | Used for generative modeling, data generation, and anomaly detection |\n",
    "| **Regularization**            | None                                                     | Explicit regularization to ensure the latent space follows a known distribution (e.g., Gaussian) |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52ee8e65-d119-4058-b062-a7031d02de38",
   "metadata": {},
   "source": [
    "## Derivation of the Loss Function used in Variational AutoEncoder\n",
    "### 1. Bayes' Theorem:\n",
    "$p(z|x) = \\frac{p(x|z)p(z)}{p(x)}$\n",
    "\n",
    "Where:\n",
    "- $ p(z|x) $ = Posterior\n",
    "- $ p(x|z) $ = Likelihood\n",
    "- $ p(z) $ = Prior\n",
    "- $ p(x) = \\int p(x|z)p(z) \\, dz $ = Marginal Likelihood (Evidence)\n",
    "\n",
    "### 2. Log of Marginal Likelihood:\n",
    "Taking the log:\n",
    "$\\log p(x) = \\log \\int p(x, z) \\, dz$\n",
    "\n",
    "### 3. Variational Approximation ($q(z|x)$):\n",
    "Introduce a simpler distribution $q(z|x)$ to approximate the posterior $p(z|x)$.\n",
    "\n",
    "### 4. Jensen's Inequality (Lower Bound):\n",
    "$\\log p(x) = \\log \\int \\frac{p(x, z)}{q(z|x)} q(z|x) \\, dz \\geq \\mathbb{E}_{q(z|x)} [ \\log \\frac{p(x, z)}{q(z|x)} ]$\n",
    "\n",
    "The ELBO (Evidence Lower Bound) becomes:\n",
    "$\\text{ELBO} = \\mathbb{E}_{q(z|x)} [ \\log p(x, z) - \\log q(z|x) ]$\n",
    "\n",
    "### 5. Expand the Joint Distribution:\n",
    "$p(x, z) = p(x|z)p(z)$\n",
    "\n",
    "So the ELBO becomes:\n",
    "$\\text{ELBO} = \\mathbb{E}_{q(z|x)} [ \\log p(x|z) + \\log p(z) - \\log q(z|x) ]$\n",
    "\n",
    "### 6. ELBO Formula:\n",
    "$\\text{ELBO} = \\mathbb{E}_{q(z|x)} [ \\log p(x|z) ] - D_{KL}(q(z|x) \\| p(z))$\n",
    "\n",
    "Where:\n",
    "- $\\mathbb{E}_{q(z|x)} [ \\log p(x|z) ]$ = Reconstruction Error\n",
    "- $ D_{KL}(q(z|x) \\| p(z)) $ = KL Divergence\n",
    "\n",
    "### 7. Simplification with Gaussian Assumptions:\n",
    "- **Prior**: $ p(z) = \\mathcal{N}(z; 0, I) $\n",
    "- **Variational Posterior**: $ q(z|x) = \\mathcal{N}(z; \\mu_q(x), \\Sigma_q(x)) $\n",
    "\n",
    "### 8. Reconstruction Error (Gaussian Likelihood):\n",
    "$\\mathbb{E}_{q(z|x)} [ \\log p(x|z) ] \\approx \\| x - f(z) \\|^2$\n",
    "\n",
    "Where $ f(z) $ is the decoder network (reconstruction).\n",
    "\n",
    "### 9. KL Divergence:\n",
    "$ D_{KL}(q(z|x) \\| p(z)) = \\frac{1}{2} \\sum_{i=1}^{d} \\left( \\sigma_i^2 + \\mu_{q,i}^2 - 1 - \\log \\sigma_i^2 \\right) $\n",
    "\n",
    "Where $ \\mu_q(x), \\sigma_q(x) $ are parameters of the variational posterior, and $ \\sigma_i^2 $ are diagonal variances of the approximate posterior.\n",
    "\n",
    "### 10. Final Simplified ELBO Loss with $ \\beta $:\n",
    "$ \\text{ELBO} = \\| x - f(z) \\|^2 + \\frac{\\beta}{2} \\sum_{i=1}^{d} \\left( \\sigma_i^2 + \\mu_{q,i}^2 - 1 - \\log \\sigma_i^2 \\right) $\n",
    "\n",
    "Where:\n",
    "- First term: **Reconstruction Loss** (MSE)\n",
    "- Second term: **$ \\beta $ scaled KL Divergence** (regularizes the posterior)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1480a361-f82a-4c25-8859-42fe4726b82e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "threads",
   "language": "python",
   "name": "threads"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
