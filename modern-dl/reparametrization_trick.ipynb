{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f8aaffc6-cef9-4133-b3a4-d1ed12b2bcf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample from N(0, 1): -1.4624308347702026\n",
      "Sampled z: -5.312154293060303\n",
      "Loss: -5.312154293060303\n",
      "Gradient wrt mu: 1.0\n",
      "Gradient wrt sigma: -1.4624308347702026\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "torch.manual_seed(47)\n",
    "\n",
    "# Parameters of the Gaussian distribution\n",
    "mu = torch.tensor(2., requires_grad=True)  # Mean (learnable)\n",
    "sigma = torch.tensor(5., requires_grad=True)  # std (learnable)\n",
    "\n",
    "# Reparametrization trick: z = mu + sigma * epsilon\n",
    "# where epsilon ~ N(0,1)\n",
    "epsilon = torch.randn_like(mu)  # Sample epsilon from N(0, 1)\n",
    "z = mu + sigma * epsilon  # Apply the reparametrization trick\n",
    "\n",
    "loss = z  # Will be involved in chain rule \n",
    "loss.backward()  \n",
    "\n",
    "# Output the results\n",
    "print(f\"Sample from N(0, 1): {epsilon.item()}\")\n",
    "print(f\"Sampled z: {z.item()}\")\n",
    "print(f\"Loss: {loss.item()}\")\n",
    "print(f\"Gradient wrt mu: {mu.grad.item()}\")\n",
    "print(f\"Gradient wrt sigma: {sigma.grad.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32aa96b7-4229-4c8e-9f3f-bf69cad3c433",
   "metadata": {},
   "source": [
    "- **Reparameterization Trick**: A technique to enable backpropagation through stochastic processes by making them differentiable.\n",
    "\n",
    "- **Problem**: Direct sampling from distributions like $z \\sim \\mathcal{N}(\\mu, \\sigma^2)$ is non-differentiable, blocking gradient computation.\n",
    "  \n",
    "  - **Without reparameterization**: Sampling $z$ is non-differentiable:\n",
    "    $$\n",
    "    z = \\text{Sample from } \\mathcal{N}(\\mu, \\sigma^2)\n",
    "    $$\n",
    "    No gradients can be computed for $\\mu$ or $\\sigma$.\n",
    "\n",
    "- **Solution**: Represent $z$ as $z = \\mu + \\sigma \\epsilon$, where $\\epsilon \\sim \\mathcal{N}(0, 1)$.\n",
    "  \n",
    "  - **Example**: If $\\mu = 2$, $\\sigma = 3$, and $\\epsilon = -1.5$:\n",
    "    $$\n",
    "    z = 2 + 3(-1.5) = -2.5\n",
    "    $$\n",
    "\n",
    "- **Why It Works**: The transformation $z = \\mu + \\sigma \\epsilon$ is differentiable, allowing gradient flow.\n",
    "\n",
    "  - Gradients:  \n",
    "    $$\n",
    "    \\frac{\\partial z}{\\partial \\mu} = 1, \\quad \\frac{\\partial z}{\\partial \\sigma} = \\epsilon\n",
    "    $$\n",
    "\n",
    "- **Without Reparameterization**: The sampling operation is non-differentiable, making gradient computation impossible.\n",
    "\n",
    "- **Applications**:  \n",
    "  - **VAEs**: Enables training of both encoder and decoder networks.\n",
    "  - **Generative Models**: Used in models with latent variables (e.g., GANs, RL).\n",
    "\n",
    "- **Benefit**: Makes non-differentiable operations differentiable, enabling gradient-based optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a92fafe-b353-4999-8134-7dd6d02fc481",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "threads",
   "language": "python",
   "name": "threads"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
